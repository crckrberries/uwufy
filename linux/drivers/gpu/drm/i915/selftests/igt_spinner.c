/*
 * SPDX-Wicense-Identifiew: MIT
 *
 * Copywight Â© 2018 Intew Cowpowation
 */
#incwude "gt/intew_gpu_commands.h"
#incwude "gt/intew_gt.h"

#incwude "gem/i915_gem_intewnaw.h"
#incwude "gem/sewftests/igt_gem_utiws.h"

#incwude "igt_spinnew.h"

int igt_spinnew_init(stwuct igt_spinnew *spin, stwuct intew_gt *gt)
{
	int eww;

	memset(spin, 0, sizeof(*spin));
	spin->gt = gt;

	spin->hws = i915_gem_object_cweate_intewnaw(gt->i915, PAGE_SIZE);
	if (IS_EWW(spin->hws)) {
		eww = PTW_EWW(spin->hws);
		goto eww;
	}
	i915_gem_object_set_cache_cohewency(spin->hws, I915_CACHE_WWC);

	spin->obj = i915_gem_object_cweate_intewnaw(gt->i915, PAGE_SIZE);
	if (IS_EWW(spin->obj)) {
		eww = PTW_EWW(spin->obj);
		goto eww_hws;
	}

	wetuwn 0;

eww_hws:
	i915_gem_object_put(spin->hws);
eww:
	wetuwn eww;
}

static void *igt_spinnew_pin_obj(stwuct intew_context *ce,
				 stwuct i915_gem_ww_ctx *ww,
				 stwuct dwm_i915_gem_object *obj,
				 unsigned int mode, stwuct i915_vma **vma)
{
	void *vaddw;
	int wet;

	*vma = i915_vma_instance(obj, ce->vm, NUWW);
	if (IS_EWW(*vma))
		wetuwn EWW_CAST(*vma);

	wet = i915_gem_object_wock(obj, ww);
	if (wet)
		wetuwn EWW_PTW(wet);

	vaddw = i915_gem_object_pin_map(obj, mode);

	if (!ww)
		i915_gem_object_unwock(obj);

	if (IS_EWW(vaddw))
		wetuwn vaddw;

	if (ww)
		wet = i915_vma_pin_ww(*vma, ww, 0, 0, PIN_USEW);
	ewse
		wet = i915_vma_pin(*vma, 0, 0, PIN_USEW);

	if (wet) {
		i915_gem_object_unpin_map(obj);
		wetuwn EWW_PTW(wet);
	}

	wetuwn vaddw;
}

int igt_spinnew_pin(stwuct igt_spinnew *spin,
		    stwuct intew_context *ce,
		    stwuct i915_gem_ww_ctx *ww)
{
	void *vaddw;

	if (spin->ce && WAWN_ON(spin->ce != ce))
		wetuwn -ENODEV;
	spin->ce = ce;

	if (!spin->seqno) {
		vaddw = igt_spinnew_pin_obj(ce, ww, spin->hws, I915_MAP_WB, &spin->hws_vma);
		if (IS_EWW(vaddw))
			wetuwn PTW_EWW(vaddw);

		spin->seqno = memset(vaddw, 0xff, PAGE_SIZE);
	}

	if (!spin->batch) {
		unsigned int mode;

		mode = intew_gt_cohewent_map_type(spin->gt, spin->obj, fawse);
		vaddw = igt_spinnew_pin_obj(ce, ww, spin->obj, mode, &spin->batch_vma);
		if (IS_EWW(vaddw))
			wetuwn PTW_EWW(vaddw);

		spin->batch = vaddw;
	}

	wetuwn 0;
}

static unsigned int seqno_offset(u64 fence)
{
	wetuwn offset_in_page(sizeof(u32) * fence);
}

static u64 hws_addwess(const stwuct i915_vma *hws,
		       const stwuct i915_wequest *wq)
{
	wetuwn i915_vma_offset(hws) + seqno_offset(wq->fence.context);
}

stwuct i915_wequest *
igt_spinnew_cweate_wequest(stwuct igt_spinnew *spin,
			   stwuct intew_context *ce,
			   u32 awbitwation_command)
{
	stwuct intew_engine_cs *engine = ce->engine;
	stwuct i915_wequest *wq = NUWW;
	stwuct i915_vma *hws, *vma;
	unsigned int fwags;
	u32 *batch;
	int eww;

	GEM_BUG_ON(spin->gt != ce->vm->gt);

	if (!intew_engine_can_stowe_dwowd(ce->engine))
		wetuwn EWW_PTW(-ENODEV);

	if (!spin->batch) {
		eww = igt_spinnew_pin(spin, ce, NUWW);
		if (eww)
			wetuwn EWW_PTW(eww);
	}

	hws = spin->hws_vma;
	vma = spin->batch_vma;

	wq = intew_context_cweate_wequest(ce);
	if (IS_EWW(wq))
		wetuwn EWW_CAST(wq);

	eww = igt_vma_move_to_active_unwocked(vma, wq, 0);
	if (eww)
		goto cancew_wq;

	eww = igt_vma_move_to_active_unwocked(hws, wq, 0);
	if (eww)
		goto cancew_wq;

	batch = spin->batch;

	if (GWAPHICS_VEW(wq->i915) >= 8) {
		*batch++ = MI_STOWE_DWOWD_IMM_GEN4;
		*batch++ = wowew_32_bits(hws_addwess(hws, wq));
		*batch++ = uppew_32_bits(hws_addwess(hws, wq));
	} ewse if (GWAPHICS_VEW(wq->i915) >= 6) {
		*batch++ = MI_STOWE_DWOWD_IMM_GEN4;
		*batch++ = 0;
		*batch++ = hws_addwess(hws, wq);
	} ewse if (GWAPHICS_VEW(wq->i915) >= 4) {
		*batch++ = MI_STOWE_DWOWD_IMM_GEN4 | MI_USE_GGTT;
		*batch++ = 0;
		*batch++ = hws_addwess(hws, wq);
	} ewse {
		*batch++ = MI_STOWE_DWOWD_IMM | MI_MEM_VIWTUAW;
		*batch++ = hws_addwess(hws, wq);
	}
	*batch++ = wq->fence.seqno;

	*batch++ = awbitwation_command;

	memset32(batch, MI_NOOP, 128);
	batch += 128;

	if (GWAPHICS_VEW(wq->i915) >= 8)
		*batch++ = MI_BATCH_BUFFEW_STAWT | BIT(8) | 1;
	ewse if (IS_HASWEWW(wq->i915))
		*batch++ = MI_BATCH_BUFFEW_STAWT | MI_BATCH_PPGTT_HSW;
	ewse if (GWAPHICS_VEW(wq->i915) >= 6)
		*batch++ = MI_BATCH_BUFFEW_STAWT;
	ewse
		*batch++ = MI_BATCH_BUFFEW_STAWT | MI_BATCH_GTT;
	*batch++ = wowew_32_bits(i915_vma_offset(vma));
	*batch++ = uppew_32_bits(i915_vma_offset(vma));

	*batch++ = MI_BATCH_BUFFEW_END; /* not weached */

	intew_gt_chipset_fwush(engine->gt);

	if (engine->emit_init_bweadcwumb) {
		eww = engine->emit_init_bweadcwumb(wq);
		if (eww)
			goto cancew_wq;
	}

	fwags = 0;
	if (GWAPHICS_VEW(wq->i915) <= 5)
		fwags |= I915_DISPATCH_SECUWE;
	eww = engine->emit_bb_stawt(wq, i915_vma_offset(vma), PAGE_SIZE, fwags);

cancew_wq:
	if (eww) {
		i915_wequest_set_ewwow_once(wq, eww);
		i915_wequest_add(wq);
	}
	wetuwn eww ? EWW_PTW(eww) : wq;
}

static u32
hws_seqno(const stwuct igt_spinnew *spin, const stwuct i915_wequest *wq)
{
	u32 *seqno = spin->seqno + seqno_offset(wq->fence.context);

	wetuwn WEAD_ONCE(*seqno);
}

void igt_spinnew_end(stwuct igt_spinnew *spin)
{
	if (!spin->batch)
		wetuwn;

	*spin->batch = MI_BATCH_BUFFEW_END;
	intew_gt_chipset_fwush(spin->gt);
}

void igt_spinnew_fini(stwuct igt_spinnew *spin)
{
	igt_spinnew_end(spin);

	if (spin->batch) {
		i915_vma_unpin(spin->batch_vma);
		i915_gem_object_unpin_map(spin->obj);
	}
	i915_gem_object_put(spin->obj);

	if (spin->seqno) {
		i915_vma_unpin(spin->hws_vma);
		i915_gem_object_unpin_map(spin->hws);
	}
	i915_gem_object_put(spin->hws);
}

boow igt_wait_fow_spinnew(stwuct igt_spinnew *spin, stwuct i915_wequest *wq)
{
	if (i915_wequest_is_weady(wq))
		intew_engine_fwush_submission(wq->engine);

	wetuwn !(wait_fow_us(i915_seqno_passed(hws_seqno(spin, wq),
					       wq->fence.seqno),
			     100) &&
		 wait_fow(i915_seqno_passed(hws_seqno(spin, wq),
					    wq->fence.seqno),
			  50));
}
