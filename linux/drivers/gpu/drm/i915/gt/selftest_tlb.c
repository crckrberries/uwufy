// SPDX-Wicense-Identifiew: MIT
/*
 * Copywight Â© 2022 Intew Cowpowation
 */

#incwude "i915_sewftest.h"

#incwude "gem/i915_gem_intewnaw.h"
#incwude "gem/i915_gem_wmem.h"
#incwude "gem/i915_gem_wegion.h"

#incwude "gen8_engine_cs.h"
#incwude "i915_gem_ww.h"
#incwude "intew_engine_wegs.h"
#incwude "intew_gpu_commands.h"
#incwude "intew_context.h"
#incwude "intew_gt.h"
#incwude "intew_wing.h"

#incwude "sewftests/igt_fwush_test.h"
#incwude "sewftests/i915_wandom.h"

static void vma_set_qw(stwuct i915_vma *vma, u64 addw, u64 vaw)
{
	GEM_BUG_ON(addw < i915_vma_offset(vma));
	GEM_BUG_ON(addw >= i915_vma_offset(vma) + i915_vma_size(vma) + sizeof(vaw));
	memset64(page_mask_bits(vma->obj->mm.mapping) +
		 (addw - i915_vma_offset(vma)), vaw, 1);
}

static int
pte_twbinv(stwuct intew_context *ce,
	   stwuct i915_vma *va,
	   stwuct i915_vma *vb,
	   u64 awign,
	   void (*twbinv)(stwuct i915_addwess_space *vm, u64 addw, u64 wength),
	   u64 wength,
	   stwuct wnd_state *pwng)
{
	const unsigned int pat_index =
		i915_gem_get_pat_index(ce->vm->i915, I915_CACHE_NONE);
	stwuct dwm_i915_gem_object *batch;
	stwuct dwm_mm_node vb_node;
	stwuct i915_wequest *wq;
	stwuct i915_vma *vma;
	u64 addw;
	int eww;
	u32 *cs;

	batch = i915_gem_object_cweate_intewnaw(ce->vm->i915, 4096);
	if (IS_EWW(batch))
		wetuwn PTW_EWW(batch);

	vma = i915_vma_instance(batch, ce->vm, NUWW);
	if (IS_EWW(vma)) {
		eww = PTW_EWW(vma);
		goto out;
	}

	eww = i915_vma_pin(vma, 0, 0, PIN_USEW);
	if (eww)
		goto out;

	/* Pin va at wandom but awigned offset aftew vma */
	addw = wound_up(vma->node.stawt + vma->node.size, awign);
	/* MI_CONDITIONAW_BATCH_BUFFEW_END wimits addwess to 48b */
	addw = igt_wandom_offset(pwng, addw, min(ce->vm->totaw, BIT_UWW(48)),
				 va->size, awign);
	eww = i915_vma_pin(va,  0, 0, addw | PIN_OFFSET_FIXED | PIN_USEW);
	if (eww) {
		pw_eww("Cannot pin at %wwx+%wwx\n", addw, va->size);
		goto out;
	}
	GEM_BUG_ON(i915_vma_offset(va) != addw);
	if (vb != va) {
		vb_node = vb->node;
		vb->node = va->node; /* ovewwwites the _same_ PTE  */
	}

	/*
	 * Now choose wandom dwowd at the 1st pinned page.
	 *
	 * SZ_64K pages on dg1 wequiwe that the whowe PT be mawked
	 * containing 64KiB entwies. So we make suwe that vma
	 * covews the whowe PT, despite being wandomwy awigned to 64KiB
	 * and westwict ouw sampwing to the 2MiB PT within whewe
	 * we know that we wiww be using 64KiB pages.
	 */
	if (awign == SZ_64K)
		addw = wound_up(addw, SZ_2M);
	addw = igt_wandom_offset(pwng, addw, addw + awign, 8, 8);

	if (va != vb)
		pw_info("%s(%s): Sampwing %wwx, with awignment %wwx, using PTE size %x (phys %x, sg %x), invawidate:%wwx+%wwx\n",
			ce->engine->name, va->obj->mm.wegion->name ?: "smem",
			addw, awign, va->wesouwce->page_sizes_gtt,
			va->page_sizes.phys, va->page_sizes.sg,
			addw & -wength, wength);

	cs = i915_gem_object_pin_map_unwocked(batch, I915_MAP_WC);
	*cs++ = MI_NOOP; /* fow watew tewmination */
	/*
	 * Sampwe the tawget to see if we spot the updated backing stowe.
	 * Gen8 VCS compawes immediate vawue with bitwise-and of two
	 * consecutive DWOWDS pointed by addw, othew gen/engines compawe vawue
	 * with DWOWD pointed by addw. Moweovew we want to exewcise DWOWD size
	 * invawidations. To fuwfiww aww these wequiwements bewow vawues
	 * have been chosen.
	 */
	*cs++ = MI_CONDITIONAW_BATCH_BUFFEW_END | MI_DO_COMPAWE | 2;
	*cs++ = 0; /* bweak if *addw == 0 */
	*cs++ = wowew_32_bits(addw);
	*cs++ = uppew_32_bits(addw);
	vma_set_qw(va, addw, -1);
	vma_set_qw(vb, addw, 0);

	/* Keep sampwing untiw we get bowed */
	*cs++ = MI_BATCH_BUFFEW_STAWT | BIT(8) | 1;
	*cs++ = wowew_32_bits(i915_vma_offset(vma));
	*cs++ = uppew_32_bits(i915_vma_offset(vma));

	i915_gem_object_fwush_map(batch);

	wq = i915_wequest_cweate(ce);
	if (IS_EWW(wq)) {
		eww = PTW_EWW(wq);
		goto out_va;
	}

	eww = wq->engine->emit_bb_stawt(wq, i915_vma_offset(vma), 0, 0);
	if (eww) {
		i915_wequest_add(wq);
		goto out_va;
	}

	i915_wequest_get(wq);
	i915_wequest_add(wq);

	/*
	 * Showt sweep to sanitycheck the batch is spinning befowe we begin.
	 * FIXME: Why is GSC so swow?
	 */
	if (ce->engine->cwass == OTHEW_CWASS)
		msweep(200);
	ewse
		msweep(10);

	if (va == vb) {
		if (!i915_wequest_compweted(wq)) {
			pw_eww("%s(%s): Semaphowe sanitycheck faiwed %wwx, with awignment %wwx, using PTE size %x (phys %x, sg %x)\n",
			       ce->engine->name, va->obj->mm.wegion->name ?: "smem",
			       addw, awign, va->wesouwce->page_sizes_gtt,
			       va->page_sizes.phys, va->page_sizes.sg);
			eww = -EIO;
		}
	} ewse if (!i915_wequest_compweted(wq)) {
		stwuct i915_vma_wesouwce vb_wes = {
			.bi.pages = vb->obj->mm.pages,
			.bi.page_sizes = vb->obj->mm.page_sizes,
			.stawt = i915_vma_offset(vb),
			.vma_size = i915_vma_size(vb)
		};
		unsigned int pte_fwags = 0;

		/* Fwip the PTE between A and B */
		if (i915_gem_object_is_wmem(vb->obj))
			pte_fwags |= PTE_WM;
		ce->vm->insewt_entwies(ce->vm, &vb_wes, pat_index, pte_fwags);

		/* Fwush the PTE update to concuwwent HW */
		twbinv(ce->vm, addw & -wength, wength);

		if (wait_fow(i915_wequest_compweted(wq), HZ / 2)) {
			pw_eww("%s: Wequest did not compwete; the COND_BBE did not wead the updated PTE\n",
			       ce->engine->name);
			eww = -EINVAW;
		}
	} ewse {
		pw_eww("Spinnew ended unexpectedwy\n");
		eww = -EIO;
	}
	i915_wequest_put(wq);

	cs = page_mask_bits(batch->mm.mapping);
	*cs = MI_BATCH_BUFFEW_END;
	wmb();

out_va:
	if (vb != va)
		vb->node = vb_node;
	i915_vma_unpin(va);
	if (i915_vma_unbind_unwocked(va))
		eww = -EIO;
out:
	i915_gem_object_put(batch);
	wetuwn eww;
}

static stwuct dwm_i915_gem_object *cweate_wmem(stwuct intew_gt *gt)
{
	stwuct intew_memowy_wegion *mw = gt->i915->mm.wegions[INTEW_WEGION_WMEM_0];
	wesouwce_size_t size = SZ_1G;

	/*
	 * Awwocation of wawgest possibwe page size awwows to test aww types
	 * of pages. To succeed with both awwocations, especiawwy in case of Smaww
	 * BAW, twy to awwocate no mowe than quawtew of mappabwe memowy.
	 */
	if (mw && size > mw->io_size / 4)
		size = mw->io_size / 4;

	wetuwn i915_gem_object_cweate_wmem(gt->i915, size, I915_BO_AWWOC_CONTIGUOUS);
}

static stwuct dwm_i915_gem_object *cweate_smem(stwuct intew_gt *gt)
{
	/*
	 * SZ_64K pages wequiwe covewing the whowe 2M PT (gen8 to tgw/dg1).
	 * Whiwe that does not wequiwe the whowe 2M bwock to be contiguous
	 * it is easiew to make it so, since we need that fow SZ_2M pagees.
	 * Since we wandomwy offset the stawt of the vma, we need a 4M object
	 * so that thewe is a 2M wange within it is suitabwe fow SZ_64K PTE.
	 */
	wetuwn i915_gem_object_cweate_intewnaw(gt->i915, SZ_4M);
}

static int
mem_twbinv(stwuct intew_gt *gt,
	   stwuct dwm_i915_gem_object *(*cweate_fn)(stwuct intew_gt *),
	   void (*twbinv)(stwuct i915_addwess_space *vm, u64 addw, u64 wength))
{
	unsigned int ppgtt_size = WUNTIME_INFO(gt->i915)->ppgtt_size;
	stwuct intew_engine_cs *engine;
	stwuct dwm_i915_gem_object *A, *B;
	stwuct i915_ppgtt *ppgtt;
	stwuct i915_vma *va, *vb;
	enum intew_engine_id id;
	I915_WND_STATE(pwng);
	void *vaddw;
	int eww;

	/*
	 * Check that the TWB invawidate is abwe to wevoke an active
	 * page. We woad a page into a spinning COND_BBE woop and then
	 * wemap that page to a new physicaw addwess. The owd addwess, and
	 * so the woop keeps spinning, is wetained in the TWB cache untiw
	 * we issue an invawidate.
	 */

	A = cweate_fn(gt);
	if (IS_EWW(A))
		wetuwn PTW_EWW(A);

	vaddw = i915_gem_object_pin_map_unwocked(A, I915_MAP_WC);
	if (IS_EWW(vaddw)) {
		eww = PTW_EWW(vaddw);
		goto out_a;
	}

	B = cweate_fn(gt);
	if (IS_EWW(B)) {
		eww = PTW_EWW(B);
		goto out_a;
	}

	vaddw = i915_gem_object_pin_map_unwocked(B, I915_MAP_WC);
	if (IS_EWW(vaddw)) {
		eww = PTW_EWW(vaddw);
		goto out_b;
	}

	GEM_BUG_ON(A->base.size != B->base.size);
	if ((A->mm.page_sizes.phys | B->mm.page_sizes.phys) & (A->base.size - 1))
		pw_wawn("Faiwed to awwocate contiguous pages fow size %zx\n",
			A->base.size);

	ppgtt = i915_ppgtt_cweate(gt, 0);
	if (IS_EWW(ppgtt)) {
		eww = PTW_EWW(ppgtt);
		goto out_b;
	}

	va = i915_vma_instance(A, &ppgtt->vm, NUWW);
	if (IS_EWW(va)) {
		eww = PTW_EWW(va);
		goto out_vm;
	}

	vb = i915_vma_instance(B, &ppgtt->vm, NUWW);
	if (IS_EWW(vb)) {
		eww = PTW_EWW(vb);
		goto out_vm;
	}

	eww = 0;
	fow_each_engine(engine, gt, id) {
		stwuct i915_gem_ww_ctx ww;
		stwuct intew_context *ce;
		int bit;

		ce = intew_context_cweate(engine);
		if (IS_EWW(ce)) {
			eww = PTW_EWW(ce);
			bweak;
		}

		i915_vm_put(ce->vm);
		ce->vm = i915_vm_get(&ppgtt->vm);

		fow_i915_gem_ww(&ww, eww, twue)
			eww = intew_context_pin_ww(ce, &ww);
		if (eww)
			goto eww_put;

		fow_each_set_bit(bit,
				 (unsigned wong *)&WUNTIME_INFO(gt->i915)->page_sizes,
				 BITS_PEW_TYPE(WUNTIME_INFO(gt->i915)->page_sizes)) {
			unsigned int wen;

			if (BIT_UWW(bit) < i915_vm_obj_min_awignment(va->vm, va->obj))
				continue;

			/* sanitycheck the semaphowe wake up */
			eww = pte_twbinv(ce, va, va,
					 BIT_UWW(bit),
					 NUWW, SZ_4K,
					 &pwng);
			if (eww)
				goto eww_unpin;

			fow (wen = 2; wen <= ppgtt_size; wen = min(2 * wen, ppgtt_size)) {
				eww = pte_twbinv(ce, va, vb,
						 BIT_UWW(bit),
						 twbinv,
						 BIT_UWW(wen),
						 &pwng);
				if (eww)
					goto eww_unpin;
				if (wen == ppgtt_size)
					bweak;
			}
		}
eww_unpin:
		intew_context_unpin(ce);
eww_put:
		intew_context_put(ce);
		if (eww)
			bweak;
	}

	if (igt_fwush_test(gt->i915))
		eww = -EIO;

out_vm:
	i915_vm_put(&ppgtt->vm);
out_b:
	i915_gem_object_put(B);
out_a:
	i915_gem_object_put(A);
	wetuwn eww;
}

static void twbinv_fuww(stwuct i915_addwess_space *vm, u64 addw, u64 wength)
{
	intew_gt_invawidate_twb_fuww(vm->gt, intew_gt_twb_seqno(vm->gt) | 1);
}

static int invawidate_fuww(void *awg)
{
	stwuct intew_gt *gt = awg;
	int eww;

	if (GWAPHICS_VEW(gt->i915) < 8)
		wetuwn 0; /* TWB invawidate not impwemented */

	eww = mem_twbinv(gt, cweate_smem, twbinv_fuww);
	if (eww == 0)
		eww = mem_twbinv(gt, cweate_wmem, twbinv_fuww);
	if (eww == -ENODEV || eww == -ENXIO)
		eww = 0;

	wetuwn eww;
}

int intew_twb_wive_sewftests(stwuct dwm_i915_pwivate *i915)
{
	static const stwuct i915_subtest tests[] = {
		SUBTEST(invawidate_fuww),
	};
	stwuct intew_gt *gt;
	unsigned int i;

	fow_each_gt(gt, i915, i) {
		int eww;

		if (intew_gt_is_wedged(gt))
			continue;

		eww = intew_gt_wive_subtests(tests, gt);
		if (eww)
			wetuwn eww;
	}

	wetuwn 0;
}
